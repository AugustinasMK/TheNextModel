{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "608a2a3f-31fc-4f72-bc14-7aec449a6c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import augly.image as imaugs\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoModel, AutoFeatureExtractor\n",
    "from utils.disc21 import DISC21Definition, DISC21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b56c35a-c860-42b2-b897-d7bd5f98502d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at google/vit-large-patch16-224 were not used when initializing ViTModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-large-patch16-224 and are newly initialized: ['vit.pooler.dense.weight', 'vit.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_ckpt = \"google/vit-large-patch16-224\"\n",
    "extractor = AutoFeatureExtractor.from_pretrained(model_ckpt)\n",
    "model = AutoModel.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce96ba34-a365-4750-8887-bf04b7f53f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformation_chain = transforms.Compose(\n",
    "    [\n",
    "        # We first resize the input image to 256x256, and then we take center crop.\n",
    "        transforms.Resize(int((256 / 224) * 224)),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=extractor.image_mean, std=extractor.image_std),\n",
    "    ]\n",
    ")\n",
    "\n",
    "augmentation_chain = transforms.Compose(\n",
    "    [\n",
    "        imaugs.Brightness(factor=2.0),\n",
    "        imaugs.RandomRotation(),\n",
    "        imaugs.OneOf([\n",
    "            imaugs.RandomAspectRatio(),\n",
    "            imaugs.RandomBlur(),\n",
    "            imaugs.RandomBrightness(),\n",
    "            imaugs.RandomNoise(),\n",
    "            imaugs.RandomPixelization(),\n",
    "        ]),\n",
    "        # We first resize the input image to 256x256, and then we take center crop.\n",
    "        transforms.Resize(int((256 / 224) * 224)),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=extractor.image_mean, std=extractor.image_std),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c1b4b50-786d-4c17-8315-c0463ed4b033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DISC21Definition dataset loaded\n",
      "  subset   | # ids | # images\n",
      "  ---------------------------\n",
      "  train    | 100000 |   100000\n",
      "  gallery  | 100000 |   100000\n",
      "  query    |  10000 |    10000\n"
     ]
    }
   ],
   "source": [
    "train_df = DISC21Definition('/scratch/lustre/home/auma4493/images/DISC21')\n",
    "train_ds = DISC21(train_df, subset='train', transform=transformation_chain, augmentations=augmentation_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d006a49e-ed4b-4a6f-9c25-90c607176bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dims = 2\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95d7ac64-75d3-4333-aad8-273e94cddbc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13c92caa-c144-41d2-8f10-29c667886b85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTModel(\n",
       "  (embeddings): ViTEmbeddings(\n",
       "    (patch_embeddings): ViTPatchEmbeddings(\n",
       "      (projection): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (encoder): ViTEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-23): 24 x ViTLayer(\n",
       "        (attention): ViTAttention(\n",
       "          (attention): ViTSelfAttention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): ViTSelfOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): ViTIntermediate(\n",
       "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): ViTOutput(\n",
       "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layernorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "  (pooler): ViTPooler(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dcdee928-8aac-4dee-9ce7-878f0161c460",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_count = 10  # for now\n",
    "lr = 1e-5  # could use a scheduler\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, )\n",
    "loss_func = torch.nn.TripletMarginLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3213f3b-3ee1-42ed-9f72-75e9a1b117b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ca4fc7f19634819991ea139255970c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9132d95c5fd4451c919f3cc562acdabe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/6250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.1643,  0.2702,  0.3025,  ..., -0.0909,  0.6494,  1.3829],\n",
      "         [ 0.9080,  0.4086,  0.6673,  ..., -1.0627,  0.5136, -0.0792],\n",
      "         [ 1.2989, -0.0561,  0.9638,  ..., -0.5098,  0.4882, -0.4838],\n",
      "         ...,\n",
      "         [ 0.5335,  0.4966, -0.3425,  ...,  0.2432, -0.8150, -0.4187],\n",
      "         [ 0.5823,  0.4559, -0.8236,  ...,  0.1264, -1.0074, -0.0441],\n",
      "         [ 0.6399,  0.7584, -1.0133,  ...,  0.2388, -0.8157, -0.3999]],\n",
      "\n",
      "        [[-0.5691,  1.4298,  0.6242,  ..., -0.2342,  0.1633,  0.7874],\n",
      "         [ 0.3702, -0.0602,  0.7627,  ...,  0.1139, -0.6421,  0.2637],\n",
      "         [ 0.4417, -0.1337,  1.1589,  ..., -0.1497, -0.6205,  0.4328],\n",
      "         ...,\n",
      "         [ 1.5070,  0.2695,  0.6197,  ..., -0.1580,  0.3269,  0.6150],\n",
      "         [ 1.6072, -0.0199, -0.1424,  ...,  0.2055, -0.1020,  0.4769],\n",
      "         [-0.2224,  0.5415, -0.6262,  ...,  0.2407, -1.3835, -1.3767]],\n",
      "\n",
      "        [[-0.5470, -1.1812, -0.1814,  ...,  1.6395,  1.4345,  1.4156],\n",
      "         [-1.3079, -0.1415,  0.9537,  ..., -0.2382,  0.0973, -0.8719],\n",
      "         [ 0.4552,  0.3724,  1.4636,  ..., -0.6029, -0.3999,  0.0786],\n",
      "         ...,\n",
      "         [ 0.1823, -0.0597,  0.3578,  ...,  0.7050,  0.2405, -0.7428],\n",
      "         [-0.3120, -0.6653, -0.0226,  ...,  1.1044,  0.0355, -0.5544],\n",
      "         [ 0.3099,  0.3370,  0.1229,  ..., -0.4153,  0.7404, -0.8355]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.2208,  1.0250, -0.1352,  ...,  0.8486, -0.0698, -0.7606],\n",
      "         [-0.3263,  0.8162,  0.2688,  ..., -1.8595,  1.7709,  0.9241],\n",
      "         [-0.2532,  0.7945,  0.2134,  ..., -1.3665,  1.7841,  0.9123],\n",
      "         ...,\n",
      "         [ 0.1905, -0.0197,  0.1670,  ...,  0.0320,  0.0102,  2.1548],\n",
      "         [-0.3059, -0.2298,  0.0720,  ...,  0.3283, -0.2856,  1.9810],\n",
      "         [ 1.2327,  0.8326,  0.3350,  ..., -0.7721,  0.1284, -0.0903]],\n",
      "\n",
      "        [[ 0.4155, -0.1502, -0.4801,  ..., -0.1815, -0.1444,  0.5000],\n",
      "         [ 0.1796, -0.6333,  0.1411,  ..., -0.6232, -0.0909,  1.1434],\n",
      "         [-0.6227, -0.4389, -0.2659,  ...,  1.0377, -0.4818,  0.4188],\n",
      "         ...,\n",
      "         [ 0.3364,  0.8468, -0.7548,  ..., -0.4816, -0.6044,  0.2428],\n",
      "         [ 0.1890,  0.3704,  0.4324,  ..., -0.3046, -0.2494, -0.3172],\n",
      "         [ 0.6966,  0.7563, -1.3847,  ..., -0.3808, -0.4667,  1.9378]],\n",
      "\n",
      "        [[-0.5297,  0.5904, -0.3136,  ..., -0.6363, -0.6019,  0.8224],\n",
      "         [-0.7336,  0.0427, -0.6959,  ...,  0.1126, -0.6097,  0.3030],\n",
      "         [ 0.5754,  1.4275, -0.9885,  ..., -1.0530,  0.1319,  0.4871],\n",
      "         ...,\n",
      "         [ 0.1436,  0.7492, -1.3897,  ..., -0.7241, -0.1331,  0.7088],\n",
      "         [ 0.3353,  1.0936, -0.5628,  ..., -1.0542, -0.1121,  0.8745],\n",
      "         [ 0.2541,  1.1547,  0.0633,  ..., -1.1001,  0.1882,  0.6429]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "Epoch: 1/10 - Loss: 1.3795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fbacb6eee50>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1443, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/popen_fork.py\", line 44, in wait\n",
      "    if not wait([self.sentinel], timeout):\n",
      "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 931, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/usr/lib/python3.8/selectors.py\", line 415, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6041ba463363456d80a4c7e20190d369",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/6250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in tqdm(range(epoch_count), desc=\"Epochs\"):\n",
    "    running_loss = []\n",
    "    for step, (anchor_img, positive_img, negative_img, anchor_label) in enumerate(\n",
    "            tqdm(train_loader, desc=\"Training\", leave=False)):\n",
    "        \n",
    "        anchor_img = anchor_img.to(device)\n",
    "        positive_img = positive_img.to(device)\n",
    "        negative_img = negative_img.to(device)\n",
    "\n",
    "        anchor_out = model(anchor_img).last_hidden_state        \n",
    "        positive_out = model(positive_img).last_hidden_state\n",
    "        negative_out = model(negative_img).last_hidden_state\n",
    "        \n",
    "        print(anchor_out)\n",
    "\n",
    "        loss = loss_func(anchor_out, positive_out, negative_out)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        running_loss.append(loss.cpu().detach().numpy())\n",
    "        break\n",
    "    print(\"Epoch: {}/{} - Loss: {:.4f}\".format(epoch + 1, epoch_count, np.mean(running_loss)))\n",
    "    torch.save({\"model_state_dict\": model.state_dict(),\n",
    "                \"optimzier_state_dict\": optimizer.state_dict()\n",
    "                }, f\"vit_checkpoints/trained_model_{epoch + 1}_{epoch_count}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af5a2b7-a782-4786-af62-b8a453c3e808",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
